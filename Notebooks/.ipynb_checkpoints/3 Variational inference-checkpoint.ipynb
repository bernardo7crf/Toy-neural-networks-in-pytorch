{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the target variable is normally distributed around some mean function of the input, given by a neural network, with some variance.\n",
    "\n",
    "\\begin{equation*}\n",
    "p(y | x, \\theta) := \\mathcal{N}(\\mu = NN(x, \\theta), K(y, y') = 1)\n",
    "\\end{equation*}\n",
    "\n",
    "Put a prior on the parameters of the network\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta) := \\mathcal{N}(\\mu = 0, \\sigma^2 = 100 \\mathcal{I})\n",
    "\\end{equation*}\n",
    "\n",
    "and try to infer their posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As variational distribution, use an independent (mean-field VI) multivariate Gaussian\n",
    "\n",
    "\\begin{equation*}\n",
    "q_\\phi(\\theta) := \\mathcal{N}(\\mu_q, \\sigma_q^2 \\mathcal{I})\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\phi := (\\mu_q,\\sigma_q)$ are the variational parameters, which are randomly initialised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then optimise a sample-based approximation of the evidence lower bound (the KL divergence between the variational distribution and the posterior, with the evidence $p(y)$ dropped as it doesn' depend on $\\theta$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}_{\\theta \\sim q_\\phi}\\left[ \\log\\frac{q_\\phi(\\theta)}{p(y | x, \\theta)p(\\theta)} \\right]\n",
    "\\approx \\frac{1}{S}\\sum_{i=1}^S \\log\\frac{q_\\phi(\\theta^{(i)})}{p(y | x, \\theta^{(i)})p(\\theta^{(i)})}\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "with respect to the variational parameters, where $\\{\\theta^{(i)}\\}_{i=1}^S\\overset{i.i.d.}{\\sim} q_\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was surprisingly easy.  Proper initialisation still helps a lot I think.  But yeah, actually seems kind of easier than gradient descent..?  \n",
    "\n",
    "Seems to fit better with a prior.  Similar to regularisation right.\n",
    "\n",
    "Seems like the output distribution is just a sum of normals so I can probably find it analytically instead of sampling.  Would be cool to check how the variance develops moving away from the data too.  And explore how the prior affects the fit.\n",
    "\n",
    "I should try to show how using the full posterior as opposed to just point estimates avoids overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "from Code.vi import *\n",
    "from Code.tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make data\n",
    "x = torch.linspace(-1, 1, 101).reshape(101, 1)\n",
    "\n",
    "# Target functions, from easy to hard\n",
    "# y = torch.maximum(2 * x, torch.zeros_like(x))\n",
    "# y = x**3\n",
    "# y = np.exp(x)\n",
    "y = np.sin(np.pi * x)\n",
    "\n",
    "# Set number of neurons in hidden layer\n",
    "n_hidden=20\n",
    "\n",
    "# Initialise variational parameters\n",
    "q_mean = init_par(n_hidden, x, y)\n",
    "plot_neurons(q_mean, x, y, n_hidden, title=\"Initial neuron outputs\")\n",
    "log_q_sd = torch.log(torch.rand([3 * n_hidden + 1]))\n",
    "log_q_sd.requires_grad = True\n",
    "\n",
    "# Run variational inference\n",
    "var_inf(\n",
    "    q_mean, log_q_sd, x, y, n_hidden, n_iterations = int(2e3), plot_every = 100, \n",
    "    n_samples = 20, lr = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run variational inference\n",
    "var_inf(\n",
    "    q_mean, log_q_sd, x, y, n_hidden, n_iterations = int(2e3), plot_every = 100, \n",
    "    n_samples = 20, lr = 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
