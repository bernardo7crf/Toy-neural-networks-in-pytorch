{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified Neural ODE example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set parameters\n",
    "method = 'dopri5'\n",
    "data_size = 1000\n",
    "batch_time = 10\n",
    "batch_size = 20\n",
    "niters = 100\n",
    "test_freq = 20\n",
    "gpu = 0\n",
    "adjoint = True\n",
    "\n",
    "# Import ODE integrator depending on whether adjoint is set to true\n",
    "if adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint\n",
    "\n",
    "# Specify GPU if available, CPU otherwise\n",
    "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initial (two-D) value, times for evaluation of solution, and a matrix \n",
    "# defining the spiral gradient\n",
    "true_y0 = torch.tensor([[2., 0.]]).to(device)\n",
    "t = torch.linspace(0., 25., data_size).to(device)\n",
    "true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device)\n",
    "\n",
    "# Lambda class extending a neural network module by defining a function \n",
    "# forward that returns a matrix multiply of the cube of a vector y and a matrix A\n",
    "class Lambda(nn.Module):\n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y**3, true_A)\n",
    "\n",
    "# Find true solution to differential equation\n",
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method='dopri5')\n",
    "\n",
    "# Create batch of samples of initial values, times, and true trajectories\n",
    "def get_batch():\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(data_size - batch_time, dtype=np.int64), batch_size, replace=False))\n",
    "    batch_y0 = true_y[s]  # (M, D)\n",
    "    batch_t = t[:batch_time]  # (T)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(batch_time)], dim=0)  # (T, M, D)\n",
    "    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)\n",
    "\n",
    "# Class extending nn module with a two-layer net, tanh on the hidden, \n",
    "# initialising the weights, and a forward function that takes the cube of y and\n",
    "# passes it through the net.\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 2),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y**3)\n",
    "\n",
    "# Pass function to GPU, attache optimiser\n",
    "func = ODEFunc().to(device)\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3)\n",
    "\n",
    "# Loop over iterations using the differential equation solver (integrator) to\n",
    "# predict the output from the neural network derivative function, getting the loss, and\n",
    "# back-propogating the gradient through the ODE solver to the neural net - \n",
    "# that's all there is to it\n",
    "for itr in range(1,  niters + 1):\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch()\n",
    "    pred_y = odeint(func, batch_y0, batch_t).to(device)\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if itr %  test_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(func, true_y0, t)\n",
    "            loss = torch.mean(torch.abs(pred_y - true_y))\n",
    "            print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
